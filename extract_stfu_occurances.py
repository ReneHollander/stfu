import argparse
import os
import re
import json
import dataclasses
import pathlib
import shutil
import subprocess
import matplotlib.pyplot as plt
from tqdm import tqdm


@dataclasses.dataclass
class Token:
    text: str
    durationMs: int
    startMs: int
    endMs: int
    autogenerated: bool
    eventStartMs: int
    eventEndMs: int


fancyReplacer = str.maketrans(
    {
        **{c: " " for c in '"$!%^&:;?/\\`*_{}[]()>#+-.!$\n,'},
        **{c: c.lower() for c in "ABCDEFGHIJKLMNOPQRSTUVWXYZ"},
    }
)


def tokens(subtiles):
    autogenerated = False
    if len(subtiles["wsWinStyles"]) == 2:
        autogenerated = True

    events = subtiles["events"]
    for eventIdx, event in enumerate(subtiles["events"]):
        eventStartMs = event.get("tStartMs", 0)
        eventDurationMs = event.get("dDurationMs", 0)
        eventEndMs = eventStartMs + eventDurationMs
        segs = event.get("segs", None)
        if not segs:
            continue

        if autogenerated:
            # If the subs are autogenerated, YouTube already did a great job of exactly aligning
            # each spoken word to the right start timestamp.
            for segIdx, seg in enumerate(segs):
                t = seg["utf8"]
                if " " in t:
                    t = t.replace("[ __ ]", "fuck")
                if "\n" in t:
                    t.replace("\n", "")
                t = t.strip()
                if not t:
                    continue
                if len(segs) == segIdx + 1:
                    if eventIdx + 1 < len(events):
                        endMs = min(events[eventIdx + 1].get("tStartMs", 0), eventEndMs)
                    else:
                        endMs = eventEndMs
                else:
                    endMs = min(
                        eventStartMs + segs[segIdx + 1].get("tOffsetMs", 0), eventEndMs
                    )
                startMs = eventStartMs + seg.get("tOffsetMs", 0)
                yield Token(
                    text=t,
                    durationMs=endMs - startMs,
                    startMs=startMs,
                    endMs=endMs,
                    autogenerated=True,
                    eventStartMs=eventStartMs,
                    eventEndMs=eventEndMs,
                )
        else:
            t = segs[0]["utf8"]
            t = t.translate(fancyReplacer)
            words = t.split(" ")
            words = [word for word in words if word]
            if not words:
                continue
            msPerWord = eventDurationMs / len(words)
            for segIdx, word in enumerate(words):
                # Sadly manually subtitled videos do not have automatic subtitles available.
                # So we have to approximate the start and end time.
                startMs = eventStartMs + segIdx * msPerWord
                endMs = eventStartMs + (segIdx + 1) * msPerWord
                yield Token(
                    text=word,
                    durationMs=endMs - startMs,
                    startMs=startMs,
                    endMs=endMs,
                    autogenerated=False,
                    eventStartMs=eventStartMs,
                    eventEndMs=eventEndMs,
                )


def find_matches(subtiles, tokensPerSecond):
    it = tokens(subtiles)
    for token in it:
        tokensPerSecond.update()
        if "shut" in token.text:
            shut = token
            token = next(it)
            tokensPerSecond.update()
            if "the" in token.text:
                the = token
                token = next(it)
                tokensPerSecond.update()
                if "fuck" in token.text or "bleep" in token.text:
                    fuck = token
                    token = next(it)
                    tokensPerSecond.update()
                    if "up" in token.text:
                        up = token
                        yield (shut, the, fuck, up)


def process(
    video_id,
    subtiles,
    clips_directory,
    rough,
    skip_download,
    stfuPerSecond,
    tokensPerSecond,
):
    matches = list(find_matches(subtiles, tokensPerSecond))
    for idx, (shut, the, fuck, up) in enumerate(matches):
        if rough:
            # Add some extra safety marging to the clip when downloading rough clips.
            clipStartMs = shut.eventStartMs - 1000
            clipEndMs = up.eventEndMs + 1000
        else:
            clipStartMs = shut.startMs
            if shut.autogenerated and shut.durationMs > 1000:
                # adjust timings a bit to reduce impact of badly timed clips for autogenerated subtitles.
                clipStartMs = shut.endMs - 1000

            clipEndMs = up.endMs
            if up.autogenerated and up.durationMs > 1000:
                # adjust timings a bit to reduce impact of badly timed clips for autogenerated subtitles.
                clipEndMs = up.startMs + 1000

        clipStartS = clipStartMs / 1000.0
        clipEndS = clipEndMs / 1000.0

        tqdm.write(
            f"Getting clip {idx} from {video_id} between {clipStartS}s - {clipEndS}s (https://www.youtube.com/watch?v={video_id}&start={int(clipStartS)}): {shut.text} {the.text} {fuck.text} {up.text}"
        )
        with open(clips_directory / f"{video_id}.{idx}.json", "w") as text_file:
            text_file.write(
                json.dumps(
                    {
                        "video_id": video_id,
                        "clipStartS": clipStartS,
                        "clipEndS": clipEndS,
                        "shut": dataclasses.asdict(shut),
                        "the": dataclasses.asdict(the),
                        "fuck": dataclasses.asdict(fuck),
                        "up": dataclasses.asdict(up),
                    },
                    indent=2,
                )
            )
        if not skip_download:
            ytdlpOutput = subprocess.run(
                [
                    "yt-dlp",
                    "--cookies-from-browser",
                    "firefox",
                    "-o",
                    f'{clips_directory / f"{video_id}.{idx}.%(ext)s"}',
                    "-f",
                    "(bestvideo+bestaudio/best)",
                    "--merge-output-format",
                    "mkv",
                    "--downloader",
                    "ffmpeg",
                    "--downloader-args",
                    f"ffmpeg_i:-ss {clipStartS} -to {clipEndS}",
                    "--downloader-args",
                    "ffmpeg:-vf scale=3840x2160 -r 29.97 -c:v ffvhuff -ar 192000 -c:a pcm_s32le",
                    f"https://www.youtube.com/watch?v={video_id}",
                ],
                capture_output=True,
            )
            if ytdlpOutput.returncode != 0:
                tqdm.write(f"yt-dlp exited with {ytdlpOutput.returncode}")
                tqdm.write(ytdlpOutput.stderr.decode())
                tqdm.write(ytdlpOutput.stdout.decode())
        stfuPerSecond.update()
    return matches


def main():
    parser = argparse.ArgumentParser(description="Extract STFU occurances.")
    parser.add_argument(
        "--subtitle-directory",
        type=pathlib.Path,
        default="data/subtitles",
        dest="subtitle_directory",
        help="Directory containing subtitles to extract.",
    )
    parser.add_argument(
        "--clips-directory",
        type=pathlib.Path,
        default="data/clips",
        dest="clips_directory",
        help="Directory to store the extracted clips in.",
    )
    parser.add_argument(
        "--stop-after",
        type=int,
        default=0,
        dest="stop_after",
        help="Stop after at least this number of clips was downloaded.",
    )
    parser.add_argument(
        "--delete-clips-directory",
        type=bool,
        default=False,
        dest="delete_clips_directory",
        help="Delete the clips directory if it already exists.",
    )
    parser.add_argument(
        "--rough",
        type=bool,
        default=False,
        dest="rough",
        help="Download 5s before and after the occurance.",
    )
    parser.add_argument(
        "--skip-download",
        type=bool,
        default=False,
        dest="skip_download",
        help="Skip downloading clips.",
    )

    args = parser.parse_args()

    if args.clips_directory.exists():
        if args.delete_clips_directory:
            shutil.rmtree(args.clips_directory)
        else:
            raise FileExistsError("clips directory already exists")

    args.clips_directory.mkdir(exist_ok=True)

    matches = []
    with tqdm(desc="Tokens processed") as tokensPerSecond:
        with tqdm(desc="STFU processed") as stfuPerSecond:
            subtitle_files = list(args.subtitle_directory.glob("*.json3"))
            for subtitle_file in tqdm(subtitle_files, desc="Files processed"):
                video_id = re.findall(
                    r"([^\.]*).*\.json3", os.path.basename(subtitle_file)
                )[0]
                with open(subtitle_file, "r") as f:
                    subtiles = json.load(f)
                    matches += process(
                        video_id,
                        subtiles,
                        args.clips_directory,
                        rough=args.rough,
                        skip_download=args.skip_download,
                        stfuPerSecond=stfuPerSecond,
                        tokensPerSecond=tokensPerSecond,
                    )
                    if args.stop_after > 0 and len(matches) >= args.stop_after:
                        break

    fig, ax = plt.subplots(nrows=5, ncols=1)
    fig.set_size_inches(10, 20)
    ax1, ax2, ax3, ax4, ax5 = ax.flatten()
    ax1.hist([up.endMs - shut.startMs for (shut, the, fuck, up) in matches], bins=50)
    ax1.legend("stfu_duration_hist")

    ax2.hist([shut.endMs - shut.startMs for (shut, the, fuck, up) in matches], bins=50)
    ax2.legend("shut_duration_hist")

    ax3.hist([the.endMs - the.startMs for (shut, the, fuck, up) in matches], bins=50)
    ax3.legend("the_duration_hist")

    ax4.hist([fuck.endMs - fuck.startMs for (shut, the, fuck, up) in matches], bins=50)
    ax4.legend("fuck_duration_hist")

    ax5.hist([up.endMs - up.startMs for (shut, the, fuck, up) in matches], bins=50)
    ax5.legend("up_duration_hist")

    fig.tight_layout()
    plt.savefig("stats.png")

    print(f"found {len(matches)} shut the fuck ups")


if __name__ == "__main__":
    main()
